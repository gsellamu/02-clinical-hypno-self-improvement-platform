# ============================================================================
# E&P Assessment Enhanced Features Installation
# Adds quality scoring, pattern detection, and clinical workflow
# ============================================================================

Write-Host "üéØ Installing E&P Assessment Enhanced Features..." -ForegroundColor Cyan

$ProjectRoot = "C:\Dev\jeeth-ai\hmi-platform"
$BackendRoot = "$ProjectRoot\backend"

# ============================================================================
# STEP 1: Enhanced Service with Quality Metrics
# ============================================================================

Write-Host "`nüìù Creating enhanced suggestibility service..." -ForegroundColor Yellow

$EnhancedServiceDir = "$BackendRoot\services"
$EnhancedServiceFile = "$EnhancedServiceDir\ep_assessment_service_enhanced.py"

@"
"""
E&P Suggestibility Assessment Service - Enhanced Edition
Includes quality scoring, pattern detection, and clinical workflow
"""
import uuid
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import func
import json
import logging

from models.assessment import UserAssessment, QuestionnaireVersion
from schemas.assessment import (
    AssessmentResult,
    ScoreBreakdown,
    QualityMetrics,
    ClinicalReview
)

logger = logging.getLogger(__name__)


class EPAssessmentServiceEnhanced:
    """
    Enhanced E&P Assessment Service with:
    - Automatic quality scoring
    - Answer pattern detection
    - Clinical review workflow
    - Time tracking
    - Fraud detection
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    # ========================================================================
    # CORE ASSESSMENT LOGIC
    # ========================================================================
    
    def calculate_scores(self, answers: Dict[str, bool]) -> ScoreBreakdown:
        """Calculate E&P Suggestibility scores from questionnaire answers"""
        # Physical Suggestibility (Questions 1-18)
        q1_score = sum(1 for i in range(1, 19) if answers.get(str(i), False))
        q1_percentage = round((q1_score / 18) * 100)
        
        # Emotional Suggestibility (Questions 19-36)
        q2_score = sum(1 for i in range(19, 37) if answers.get(str(i), False))
        q2_percentage = round((q2_score / 18) * 100)
        
        # Combined
        combined_score = q1_score + q2_score
        combined_percentage = round((combined_score / 36) * 100)
        
        # Calculate percentages of each type
        if combined_score > 0:
            physical_percentage = round((q1_score / combined_score) * 100)
            emotional_percentage = round((q2_score / combined_score) * 100)
        else:
            physical_percentage = 50
            emotional_percentage = 50
        
        # Classification
        suggestibility_type = self._classify_suggestibility(
            physical_percentage, 
            emotional_percentage
        )
        
        return ScoreBreakdown(
            q1_score=q1_percentage,
            q2_score=q2_percentage,
            combined_score=combined_percentage,
            physical_percentage=physical_percentage,
            emotional_percentage=emotional_percentage,
            suggestibility_type=suggestibility_type
        )
    
    def _classify_suggestibility(
        self, 
        physical_pct: int, 
        emotional_pct: int
    ) -> str:
        """Classify suggestibility type based on HMI methodology"""
        diff = abs(physical_pct - emotional_pct)
        
        if diff <= 10:
            return "Somnambulistic (Balanced)"
        elif physical_pct > emotional_pct:
            return "Highly Physical Suggestible" if physical_pct >= 70 else "Physical Suggestible"
        else:
            return "Highly Emotional Suggestible" if emotional_pct >= 70 else "Emotional Suggestible"
    
    # ========================================================================
    # QUALITY METRICS & PATTERN DETECTION
    # ========================================================================
    
    def calculate_quality_metrics(
        self, 
        answers: Dict[str, bool],
        time_to_complete: Optional[int] = None
    ) -> QualityMetrics:
        """
        Calculate quality metrics for the assessment
        Detects patterns and assigns confidence score
        """
        # Pattern detection
        pattern = self._detect_answer_pattern(answers)
        
        # Confidence score (0-100)
        confidence = self._calculate_confidence_score(answers, time_to_complete)
        
        # Response consistency
        consistency = self._calculate_consistency(answers)
        
        # Completion percentage
        completion_pct = (len(answers) / 36) * 100
        
        # Flags for review
        needs_review = self._needs_clinical_review(
            pattern, confidence, time_to_complete
        )
        
        return QualityMetrics(
            confidence_score=confidence,
            answer_pattern=pattern,
            consistency_score=consistency,
            completion_percentage=completion_pct,
            needs_review=needs_review,
            review_reasons=self._get_review_reasons(pattern, confidence, time_to_complete)
        )
    
    def _detect_answer_pattern(self, answers: Dict[str, bool]) -> str:
        """
        Detect suspicious answer patterns
        Returns: 'balanced', 'all_yes', 'all_no', 'mostly_yes', 'mostly_no', 'alternating', 'suspicious'
        """
        if not answers:
            return "empty"
        
        values = [answers.get(str(i), False) for i in range(1, 37)]
        yes_count = sum(values)
        total = len(values)
        yes_ratio = yes_count / total if total > 0 else 0
        
        # Check for extreme patterns
        if yes_count == total:
            return "all_yes"
        elif yes_count == 0:
            return "all_no"
        elif yes_ratio > 0.85:
            return "mostly_yes"
        elif yes_ratio < 0.15:
            return "mostly_no"
        
        # Check for alternating pattern
        alternations = sum(1 for i in range(len(values)-1) if values[i] != values[i+1])
        if alternations >= len(values) * 0.9:
            return "alternating"
        
        # Check for suspicious blocks (same answer for 10+ consecutive questions)
        max_streak = self._get_max_streak(values)
        if max_streak >= 10:
            return "suspicious"
        
        return "balanced"
    
    def _get_max_streak(self, values: List[bool]) -> int:
        """Get maximum consecutive same answers"""
        if not values:
            return 0
        
        max_streak = 1
        current_streak = 1
        
        for i in range(1, len(values)):
            if values[i] == values[i-1]:
                current_streak += 1
                max_streak = max(max_streak, current_streak)
            else:
                current_streak = 1
        
        return max_streak
    
    def _calculate_confidence_score(
        self, 
        answers: Dict[str, bool],
        time_to_complete: Optional[int] = None
    ) -> float:
        """
        Calculate confidence score (0-100)
        Based on answer distribution and completion time
        """
        score = 100.0
        
        # Answer distribution factor
        yes_count = sum(1 for v in answers.values() if v)
        yes_ratio = yes_count / len(answers) if answers else 0
        
        # Penalize extreme yes/no ratios
        if yes_ratio > 0.9 or yes_ratio < 0.1:
            score -= 40
        elif yes_ratio > 0.8 or yes_ratio < 0.2:
            score -= 20
        
        # Time factor
        if time_to_complete:
            # Too fast (< 2 min = 120 sec) - likely rushing
            if time_to_complete < 120:
                score -= 30
            # Too slow (> 20 min = 1200 sec) - may be distracted
            elif time_to_complete > 1200:
                score -= 10
            # Optimal: 3-10 minutes
            elif 180 <= time_to_complete <= 600:
                score += 10
        
        # Pattern penalty
        pattern = self._detect_answer_pattern(answers)
        if pattern in ['all_yes', 'all_no', 'alternating']:
            score -= 50
        elif pattern in ['suspicious']:
            score -= 30
        
        return max(0.0, min(100.0, score))
    
    def _calculate_consistency(self, answers: Dict[str, bool]) -> float:
        """
        Calculate response consistency (0-100)
        Looks for logical contradictions in answers
        """
        # TODO: Implement consistency checks based on question pairs
        # For now, return a baseline based on pattern
        pattern = self._detect_answer_pattern(answers)
        
        if pattern == "balanced":
            return 85.0
        elif pattern in ['mostly_yes', 'mostly_no']:
            return 70.0
        else:
            return 50.0
    
    def _needs_clinical_review(
        self,
        pattern: str,
        confidence: float,
        time_to_complete: Optional[int]
    ) -> bool:
        """Determine if assessment needs clinical review"""
        # Low confidence
        if confidence < 60:
            return True
        
        # Suspicious patterns
        if pattern in ['all_yes', 'all_no', 'alternating', 'suspicious']:
            return True
        
        # Too fast completion
        if time_to_complete and time_to_complete < 90:
            return True
        
        return False
    
    def _get_review_reasons(
        self,
        pattern: str,
        confidence: float,
        time_to_complete: Optional[int]
    ) -> List[str]:
        """Get list of reasons why assessment needs review"""
        reasons = []
        
        if confidence < 60:
            reasons.append(f"Low confidence score ({confidence:.1f})")
        
        if pattern in ['all_yes', 'all_no']:
            reasons.append(f"Extreme answer pattern: {pattern}")
        elif pattern in ['alternating', 'suspicious']:
            reasons.append(f"Suspicious answer pattern: {pattern}")
        
        if time_to_complete:
            if time_to_complete < 90:
                reasons.append(f"Completed too quickly ({time_to_complete}s)")
            elif time_to_complete > 1200:
                reasons.append(f"Took unusually long ({time_to_complete}s)")
        
        return reasons
    
    # ========================================================================
    # SAVE ASSESSMENT WITH ENHANCED FEATURES
    # ========================================================================
    
    def save_assessment(
        self,
        user_id: str,
        answers: Dict[str, bool],
        session_id: Optional[str] = None,
        time_to_complete: Optional[int] = None,
        ip_address: Optional[str] = None
    ) -> AssessmentResult:
        """
        Save assessment with enhanced quality metrics
        """
        # Get active questionnaire
        questionnaire = self.db.query(QuestionnaireVersion).filter(
            QuestionnaireVersion.is_active == 1
        ).first()
        
        if not questionnaire:
            raise ValueError("No active questionnaire version found")
        
        # Calculate scores
        scores = self.calculate_scores(answers)
        
        # Calculate quality metrics
        quality = self.calculate_quality_metrics(answers, time_to_complete)
        
        # Create assessment record
        assessment = UserAssessment(
            id=uuid.uuid4(),
            user_id=uuid.UUID(user_id),
            session_id=uuid.UUID(session_id) if session_id else None,
            questionnaire_version_id=questionnaire.id,
            
            # Scores
            q1_score=scores.q1_score,
            q2_score=scores.q2_score,
            combined_score=scores.combined_score,
            physical_percentage=scores.physical_percentage,
            emotional_percentage=scores.emotional_percentage,
            suggestibility_type=scores.suggestibility_type,
            profile=scores.suggestibility_type,  # Compatibility
            
            # Raw data
            answers=answers,
            
            # Quality metrics
            confidence_score=quality.confidence_score,
            answer_pattern_signature=quality.answer_pattern,
            completion_percentage=quality.completion_percentage,
            
            # Timing
            time_to_complete_seconds=time_to_complete,
            completed_at=datetime.utcnow(),
            
            # Audit trail
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        self.db.add(assessment)
        self.db.commit()
        self.db.refresh(assessment)
        
        # Auto-flag for review if needed
        if quality.needs_review:
            self._auto_flag_for_review(assessment.id, quality.review_reasons)
        
        return AssessmentResult(
            assessment_id=assessment.id,
            user_id=assessment.user_id,
            session_id=assessment.session_id,
            questionnaire_version_id=assessment.questionnaire_version_id,
            scores=scores,
            quality_metrics=quality,
            completed_at=assessment.completed_at
        )
    
    def _auto_flag_for_review(self, assessment_id: uuid.UUID, reasons: List[str]):
        """Automatically flag assessment for clinical review"""
        assessment = self.db.query(UserAssessment).filter(
            UserAssessment.id == assessment_id
        ).first()
        
        if assessment:
            note = f"AUTO-FLAGGED: {', '.join(reasons)}"
            assessment.clinical_notes = note
            self.db.commit()
            
            logger.warning(f"Assessment {assessment_id} auto-flagged: {reasons}")
    
    # ========================================================================
    # CLINICAL REVIEW WORKFLOW
    # ========================================================================
    
    def flag_for_review(
        self,
        assessment_id: str,
        flagged_by: str,
        reason: str
    ) -> bool:
        """Manually flag assessment for clinical review"""
        assessment = self.db.query(UserAssessment).filter(
            UserAssessment.id == uuid.UUID(assessment_id)
        ).first()
        
        if not assessment:
            return False
        
        timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
        note = f"[{timestamp}] FLAGGED by {flagged_by}: {reason}"
        
        if assessment.clinical_notes:
            assessment.clinical_notes += f"\n{note}"
        else:
            assessment.clinical_notes = note
        
        assessment.updated_at = datetime.utcnow()
        self.db.commit()
        
        return True
    
    def mark_reviewed(
        self,
        assessment_id: str,
        reviewer_id: str,
        notes: Optional[str] = None,
        approved: bool = True
    ) -> bool:
        """Mark assessment as reviewed by clinician"""
        assessment = self.db.query(UserAssessment).filter(
            UserAssessment.id == uuid.UUID(assessment_id)
        ).first()
        
        if not assessment:
            return False
        
        assessment.reviewed_by = uuid.UUID(reviewer_id)
        assessment.reviewed_at = datetime.utcnow()
        
        timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
        status = "APPROVED" if approved else "REJECTED"
        note = f"[{timestamp}] {status} by {reviewer_id}"
        
        if notes:
            note += f": {notes}"
        
        if assessment.clinical_notes:
            assessment.clinical_notes += f"\n{note}"
        else:
            assessment.clinical_notes = note
        
        assessment.updated_at = datetime.utcnow()
        self.db.commit()
        
        return True
    
    def get_pending_reviews(self, limit: int = 50) -> List[AssessmentResult]:
        """Get assessments pending clinical review"""
        assessments = self.db.query(UserAssessment).filter(
            UserAssessment.confidence_score < 60,
            UserAssessment.reviewed_at.is_(None)
        ).order_by(
            UserAssessment.completed_at.desc()
        ).limit(limit).all()
        
        return [self._to_assessment_result(a) for a in assessments]
    
    # ========================================================================
    # ANALYTICS & REPORTING
    # ========================================================================
    
    def get_quality_statistics(self, days: int = 30) -> Dict:
        """Get quality statistics for assessments"""
        cutoff = datetime.utcnow() - timedelta(days=days)
        
        assessments = self.db.query(UserAssessment).filter(
            UserAssessment.completed_at >= cutoff
        ).all()
        
        if not assessments:
            return {}
        
        patterns = {}
        total_confidence = 0
        flagged_count = 0
        
        for a in assessments:
            # Count patterns
            pattern = a.answer_pattern_signature or "unknown"
            patterns[pattern] = patterns.get(pattern, 0) + 1
            
            # Sum confidence
            if a.confidence_score:
                total_confidence += a.confidence_score
            
            # Count flagged
            if a.clinical_notes and "FLAGGED" in a.clinical_notes:
                flagged_count += 1
        
        return {
            "total_assessments": len(assessments),
            "avg_confidence": total_confidence / len(assessments) if assessments else 0,
            "pattern_distribution": patterns,
            "flagged_for_review": flagged_count,
            "flagged_percentage": (flagged_count / len(assessments)) * 100 if assessments else 0,
            "period_days": days
        }
    
    def get_user_assessment_history(
        self,
        user_id: str,
        limit: int = 10
    ) -> List[AssessmentResult]:
        """Get assessment history for user"""
        assessments = self.db.query(UserAssessment).filter(
            UserAssessment.user_id == uuid.UUID(user_id)
        ).order_by(
            UserAssessment.completed_at.desc()
        ).limit(limit).all()
        
        return [self._to_assessment_result(a) for a in assessments]
    
    def _to_assessment_result(self, assessment: UserAssessment) -> AssessmentResult:
        """Convert database record to AssessmentResult"""
        scores = ScoreBreakdown(
            q1_score=assessment.q1_score,
            q2_score=assessment.q2_score,
            combined_score=assessment.combined_score,
            physical_percentage=assessment.physical_percentage,
            emotional_percentage=assessment.emotional_percentage,
            suggestibility_type=assessment.suggestibility_type or assessment.profile
        )
        
        quality = QualityMetrics(
            confidence_score=assessment.confidence_score or 0,
            answer_pattern=assessment.answer_pattern_signature or "unknown",
            consistency_score=0,  # Not stored yet
            completion_percentage=assessment.completion_percentage or 100,
            needs_review=assessment.reviewed_at is None and assessment.confidence_score < 60,
            review_reasons=[]
        )
        
        return AssessmentResult(
            assessment_id=assessment.id,
            user_id=assessment.user_id,
            session_id=assessment.session_id,
            questionnaire_version_id=assessment.questionnaire_version_id,
            scores=scores,
            quality_metrics=quality,
            completed_at=assessment.completed_at
        )
    
    # ========================================================================
    # COMMUNICATION PREFERENCES (for AI Agents)
    # ========================================================================
    
    def get_communication_preferences(self, user_id: str) -> Dict:
        """Get AI communication preferences based on latest assessment"""
        assessment = self.db.query(UserAssessment).filter(
            UserAssessment.user_id == uuid.UUID(user_id)
        ).order_by(
            UserAssessment.completed_at.desc()
        ).first()
        
        if not assessment:
            return {
                "style": "balanced",
                "confidence": 0,
                "message": "No assessment completed"
            }
        
        sug_type = assessment.suggestibility_type or assessment.profile
        confidence = assessment.confidence_score or 0
        
        base_prefs = self._get_communication_style(sug_type)
        base_prefs["confidence"] = confidence
        base_prefs["assessment_date"] = assessment.completed_at.isoformat()
        
        # Add warning if low confidence
        if confidence < 60:
            base_prefs["warning"] = "Low confidence assessment - use with caution"
        
        return base_prefs
    
    def _get_communication_style(self, sug_type: str) -> Dict:
        """Get communication style guidelines"""
        if "Physical" in sug_type:
            return {
                "style": "physical_suggestible",
                "tone": "direct and clear",
                "use_metaphors": False,
                "use_literal": True,
                "language_pattern": "literal and step-by-step",
                "example": "Take a deep breath. Hold for 3 seconds. Exhale slowly."
            }
        elif "Emotional" in sug_type:
            return {
                "style": "emotional_suggestible",
                "tone": "indirect and flowing",
                "use_metaphors": True,
                "use_literal": False,
                "language_pattern": "inferential and metaphorical",
                "example": "You might find yourself drifting like a leaf on a stream..."
            }
        else:
            return {
                "style": "somnambulistic",
                "tone": "flexible - adaptive mix",
                "use_metaphors": True,
                "use_literal": True,
                "language_pattern": "combine literal and metaphorical",
                "example": "As you breathe deeply, notice yourself relaxing more..."
            }
"@ | Out-File -FilePath $EnhancedServiceFile -Encoding UTF8

Write-Host "‚úÖ Created: $EnhancedServiceFile" -ForegroundColor Green

# ============================================================================
# STEP 2: Enhanced API Routes
# ============================================================================

Write-Host "`nüìù Creating enhanced API routes..." -ForegroundColor Yellow

$RoutesDir = "$BackendRoot\routes"
if (-not (Test-Path $RoutesDir)) {
    New-Item -ItemType Directory -Path $RoutesDir -Force | Out-Null
}

$EnhancedRoutesFile = "$RoutesDir\ep_assessment_enhanced.py"

@"
"""
E&P Assessment Enhanced API Routes
Includes quality metrics, clinical review, and analytics endpoints
"""
from fastapi import APIRouter, Depends, HTTPException, Query, Body
from sqlalchemy.orm import Session
from typing import Optional, List
from pydantic import BaseModel, Field

from database import get_db
from auth import get_current_user, require_role
from services.ep_assessment_service_enhanced import EPAssessmentServiceEnhanced
from schemas.assessment import AssessmentResult, QualityStatistics


router = APIRouter(prefix="/api/v1/ep-assessment", tags=["E&P Assessment Enhanced"])


# ============================================================================
# Request/Response Models
# ============================================================================

class SubmitAssessmentRequest(BaseModel):
    """Request model for submitting assessment"""
    answers: dict = Field(..., description="36 question answers")
    session_id: Optional[str] = Field(None, description="Optional VR session ID")
    time_to_complete: Optional[int] = Field(None, description="Time in seconds")
    
    class Config:
        json_schema_extra = {
            "example": {
                "answers": {"1": True, "2": False, "3": True},
                "session_id": "uuid-here",
                "time_to_complete": 240
            }
        }


class FlagReviewRequest(BaseModel):
    """Request to flag assessment for review"""
    reason: str = Field(..., description="Reason for flagging")


class MarkReviewedRequest(BaseModel):
    """Request to mark assessment as reviewed"""
    notes: Optional[str] = Field(None, description="Review notes")
    approved: bool = Field(True, description="Approved or rejected")


# ============================================================================
# Assessment Endpoints
# ============================================================================

@router.post("/submit", response_model=AssessmentResult)
async def submit_assessment(
    request: SubmitAssessmentRequest,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Submit E&P Suggestibility Assessment
    Automatically calculates quality metrics and flags suspicious patterns
    """
    if len(request.answers) != 36:
        raise HTTPException(400, f"Expected 36 answers, got {len(request.answers)}")
    
    service = EPAssessmentServiceEnhanced(db)
    
    try:
        result = service.save_assessment(
            user_id=current_user["id"],
            answers=request.answers,
            session_id=request.session_id,
            time_to_complete=request.time_to_complete
        )
        
        return result
        
    except ValueError as e:
        raise HTTPException(400, str(e))
    except Exception as e:
        raise HTTPException(500, f"Error saving assessment: {str(e)}")


@router.get("/results/latest", response_model=AssessmentResult)
async def get_latest_assessment(
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Get user's most recent assessment with quality metrics"""
    service = EPAssessmentServiceEnhanced(db)
    
    assessments = service.get_user_assessment_history(current_user["id"], limit=1)
    
    if not assessments:
        raise HTTPException(404, "No assessments found")
    
    return assessments[0]


@router.get("/results", response_model=List[AssessmentResult])
async def get_assessment_history(
    limit: int = Query(10, ge=1, le=50),
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Get user's assessment history"""
    service = EPAssessmentServiceEnhanced(db)
    return service.get_user_assessment_history(current_user["id"], limit)


# ============================================================================
# Clinical Review Endpoints
# ============================================================================

@router.post("/{assessment_id}/flag")
async def flag_for_review(
    assessment_id: str,
    request: FlagReviewRequest,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Flag assessment for clinical review"""
    service = EPAssessmentServiceEnhanced(db)
    
    success = service.flag_for_review(
        assessment_id=assessment_id,
        flagged_by=current_user["id"],
        reason=request.reason
    )
    
    if not success:
        raise HTTPException(404, "Assessment not found")
    
    return {"success": True, "message": "Assessment flagged for review"}


@router.post("/{assessment_id}/review")
async def mark_reviewed(
    assessment_id: str,
    request: MarkReviewedRequest,
    current_user: dict = Depends(require_role("clinician")),
    db: Session = Depends(get_db)
):
    """Mark assessment as reviewed (clinicians only)"""
    service = EPAssessmentServiceEnhanced(db)
    
    success = service.mark_reviewed(
        assessment_id=assessment_id,
        reviewer_id=current_user["id"],
        notes=request.notes,
        approved=request.approved
    )
    
    if not success:
        raise HTTPException(404, "Assessment not found")
    
    return {
        "success": True,
        "message": f"Assessment {'approved' if request.approved else 'rejected'}"
    }


@router.get("/pending-reviews", response_model=List[AssessmentResult])
async def get_pending_reviews(
    limit: int = Query(50, ge=1, le=100),
    current_user: dict = Depends(require_role("clinician")),
    db: Session = Depends(get_db)
):
    """Get assessments pending clinical review (clinicians only)"""
    service = EPAssessmentServiceEnhanced(db)
    return service.get_pending_reviews(limit)


# ============================================================================
# Analytics Endpoints
# ============================================================================

@router.get("/analytics/quality")
async def get_quality_statistics(
    days: int = Query(30, ge=1, le=365),
    current_user: dict = Depends(require_role("admin")),
    db: Session = Depends(get_db)
):
    """
    Get quality statistics for assessments
    Shows confidence scores, patterns, and flagged assessments
    """
    service = EPAssessmentServiceEnhanced(db)
    return service.get_quality_statistics(days)


@router.get("/communication-preferences")
async def get_communication_preferences(
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get AI communication preferences based on user's suggestibility type
    Used by AI agents to personalize language patterns
    """
    service = EPAssessmentServiceEnhanced(db)
    return service.get_communication_preferences(current_user["id"])
"@ | Out-File -FilePath $EnhancedRoutesFile -Encoding UTF8

Write-Host "‚úÖ Created: $EnhancedRoutesFile" -ForegroundColor Green

# ============================================================================
# STEP 3: Enhanced Schemas
# ============================================================================

Write-Host "`nüìù Creating enhanced schemas..." -ForegroundColor Yellow

$SchemasDir = "$BackendRoot\schemas"
if (-not (Test-Path $SchemasDir)) {
    New-Item -ItemType Directory -Path $SchemasDir -Force | Out-Null
}

$EnhancedSchemasFile = "$SchemasDir\assessment.py"

@"
"""
E&P Assessment Schemas - Enhanced with Quality Metrics
"""
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime
from uuid import UUID


class ScoreBreakdown(BaseModel):
    """Assessment scores breakdown"""
    q1_score: int = Field(..., description="Physical Suggestibility score (0-100)")
    q2_score: int = Field(..., description="Emotional Suggestibility score (0-100)")
    combined_score: int = Field(..., description="Combined score (0-100)")
    physical_percentage: int = Field(..., description="Physical percentage")
    emotional_percentage: int = Field(..., description="Emotional percentage")
    suggestibility_type: str = Field(..., description="Classification")


class QualityMetrics(BaseModel):
    """Quality metrics for assessment"""
    confidence_score: float = Field(..., ge=0, le=100, description="Confidence (0-100)")
    answer_pattern: str = Field(..., description="Pattern detected")
    consistency_score: float = Field(..., ge=0, le=100, description="Consistency (0-100)")
    completion_percentage: float = Field(..., ge=0, le=100, description="Completion %")
    needs_review: bool = Field(..., description="Needs clinical review")
    review_reasons: List[str] = Field(default_factory=list, description="Why it needs review")


class AssessmentResult(BaseModel):
    """Complete assessment result"""
    assessment_id: UUID
    user_id: UUID
    session_id: Optional[UUID] = None
    questionnaire_version_id: UUID
    scores: ScoreBreakdown
    quality_metrics: QualityMetrics
    completed_at: datetime
    
    class Config:
        from_attributes = True


class ClinicalReview(BaseModel):
    """Clinical review information"""
    reviewed_by: Optional[UUID] = None
    reviewed_at: Optional[datetime] = None
    clinical_notes: Optional[str] = None
    approved: bool = False


class QualityStatistics(BaseModel):
    """Quality statistics across assessments"""
    total_assessments: int
    avg_confidence: float
    pattern_distribution: dict
    flagged_for_review: int
    flagged_percentage: float
    period_days: int
"@ | Out-File -FilePath $EnhancedSchemasFile -Encoding UTF8

Write-Host "‚úÖ Created: $EnhancedSchemasFile" -ForegroundColor Green

# ============================================================================
# STEP 4: Create Test File
# ============================================================================

Write-Host "`nüìù Creating test file for enhanced features..." -ForegroundColor Yellow

$TestFile = "$BackendRoot\test_enhanced_assessment.py"

@"
"""
Test Enhanced E&P Assessment Features
"""
import pytest
from datetime import datetime
from services.ep_assessment_service_enhanced import EPAssessmentServiceEnhanced


def test_quality_metrics():
    """Test quality metric calculation"""
    service = EPAssessmentServiceEnhanced(None)
    
    # Test balanced answers
    balanced_answers = {str(i): i % 2 == 0 for i in range(1, 37)}
    quality = service.calculate_quality_metrics(balanced_answers, time_to_complete=300)
    
    assert quality.answer_pattern == "balanced"
    assert quality.confidence_score > 70
    assert not quality.needs_review
    
    # Test all yes answers
    all_yes = {str(i): True for i in range(1, 37)}
    quality = service.calculate_quality_metrics(all_yes, time_to_complete=300)
    
    assert quality.answer_pattern == "all_yes"
    assert quality.confidence_score < 60
    assert quality.needs_review
    
    # Test too fast completion
    quality = service.calculate_quality_metrics(balanced_answers, time_to_complete=60)
    assert quality.needs_review
    assert "too quickly" in str(quality.review_reasons).lower()


def test_pattern_detection():
    """Test answer pattern detection"""
    service = EPAssessmentServiceEnhanced(None)
    
    # All yes
    all_yes = {str(i): True for i in range(1, 37)}
    pattern = service._detect_answer_pattern(all_yes)
    assert pattern == "all_yes"
    
    # All no
    all_no = {str(i): False for i in range(1, 37)}
    pattern = service._detect_answer_pattern(all_no)
    assert pattern == "all_no"
    
    # Alternating
    alternating = {str(i): i % 2 == 0 for i in range(1, 37)}
    pattern = service._detect_answer_pattern(alternating)
    assert pattern == "alternating"
    
    # Balanced
    balanced = {
        **{str(i): True for i in range(1, 19)},
        **{str(i): False for i in range(19, 37)}
    }
    pattern = service._detect_answer_pattern(balanced)
    assert pattern in ["balanced", "suspicious"]


def test_confidence_calculation():
    """Test confidence score calculation"""
    service = EPAssessmentServiceEnhanced(None)
    
    # Good answers, good time
    balanced = {str(i): i % 2 == 0 for i in range(1, 37)}
    confidence = service._calculate_confidence_score(balanced, 300)
    assert confidence >= 80
    
    # All yes, too fast
    all_yes = {str(i): True for i in range(1, 37)}
    confidence = service._calculate_confidence_score(all_yes, 60)
    assert confidence < 40


if __name__ == "__main__":
    print("Testing Enhanced E&P Assessment Features...")
    test_quality_metrics()
    print("‚úÖ Quality metrics test passed")
    
    test_pattern_detection()
    print("‚úÖ Pattern detection test passed")
    
    test_confidence_calculation()
    print("‚úÖ Confidence calculation test passed")
    
    print("\nüéâ All tests passed!")
"@ | Out-File -FilePath $TestFile -Encoding UTF8

Write-Host "‚úÖ Created: $TestFile" -ForegroundColor Green

# ============================================================================
# STEP 5: Create README Documentation
# ============================================================================

Write-Host "`nüìù Creating documentation..." -ForegroundColor Yellow

$DocsFile = "$ProjectRoot\EP_ASSESSMENT_ENHANCED_FEATURES.md"

@"
# E&P Assessment Enhanced Features

## üéØ What's New

Your E&P Assessment system now includes:

### 1. **Quality Scoring** 
- Automatic confidence score (0-100) for every assessment
- Based on answer distribution and completion time
- Flags low-quality assessments automatically

### 2. **Pattern Detection**
- Detects suspicious patterns:
  - All yes / All no
  - Alternating (yes/no/yes/no...)
  - Long streaks (10+ same answers)
  - Too fast completion (< 90 seconds)

### 3. **Clinical Review Workflow**
- Auto-flag low confidence assessments
- Manual flagging by users/admins
- Clinical review by therapists
- Approve/reject with notes
- Full audit trail

### 4. **Analytics Dashboard**
- Quality statistics over time
- Pattern distribution charts
- Flagged assessment tracking
- Average confidence scores

## üìä Quality Metrics

Every assessment now includes:

\`\`\`python
{
  "quality_metrics": {
    "confidence_score": 85.0,        # How reliable (0-100)
    "answer_pattern": "balanced",    # Pattern detected
    "consistency_score": 80.0,       # Internal consistency
    "completion_percentage": 100.0,  # How much completed
    "needs_review": false,           # Auto-flagged?
    "review_reasons": []             # Why flagged
  }
}
\`\`\`

### Confidence Scoring Rules

| Score | Meaning | Action |
|-------|---------|--------|
| 90-100 | Excellent | Use with full confidence |
| 70-89 | Good | Use normally |
| 50-69 | Fair | Use with caution |
| 0-49 | Poor | Flag for review |

### Pattern Detection

| Pattern | Example | Confidence Impact |
|---------|---------|-------------------|
| balanced | Mix of yes/no | +10 points |
| all_yes | All 36 yes | -50 points |
| all_no | All 36 no | -50 points |
| alternating | yes/no/yes/no | -50 points |
| suspicious | 10+ same in row | -30 points |

### Time Scoring

| Time | Impact |
|------|--------|
| < 90 sec | -30 points (too fast, likely rushing) |
| 90-180 sec | -10 points (fast but acceptable) |
| 180-600 sec | +10 points (optimal time) |
| 600-1200 sec | 0 points (slow but ok) |
| > 1200 sec | -10 points (too slow, distracted?) |

## üöÄ New API Endpoints

### Submit Assessment (Enhanced)
\`\`\`bash
POST /api/v1/ep-assessment/submit
{
  "answers": {"1": true, "2": false, ...},
  "session_id": "uuid",
  "time_to_complete": 240  # NEW: track completion time
}

# Response includes quality metrics
{
  "scores": {...},
  "quality_metrics": {
    "confidence_score": 85,
    "answer_pattern": "balanced",
    "needs_review": false
  }
}
\`\`\`

### Flag for Review
\`\`\`bash
POST /api/v1/ep-assessment/{id}/flag
{
  "reason": "User reported feeling rushed"
}
\`\`\`

### Clinical Review
\`\`\`bash
POST /api/v1/ep-assessment/{id}/review
{
  "notes": "Reviewed with patient, results are valid",
  "approved": true
}
\`\`\`

### Get Pending Reviews
\`\`\`bash
GET /api/v1/ep-assessment/pending-reviews
# Returns all assessments flagged for review
\`\`\`

### Quality Analytics
\`\`\`bash
GET /api/v1/ep-assessment/analytics/quality?days=30
# Returns:
{
  "total_assessments": 150,
  "avg_confidence": 78.5,
  "pattern_distribution": {
    "balanced": 120,
    "all_yes": 5,
    "all_no": 3,
    "suspicious": 22
  },
  "flagged_for_review": 30,
  "flagged_percentage": 20.0
}
\`\`\`

## üí° Use Cases

### 1. Auto Quality Control

Assessments with confidence < 60 are automatically flagged:

\`\`\`python
if confidence < 60:
    flag_for_review()
    notify_clinician()
\`\`\`

### 2. Fraud Detection

Detect users gaming the system:

\`\`\`python
if pattern == "all_yes" or time < 90:
    flag_as_suspicious()
    require_retake()
\`\`\`

### 3. Clinical Dashboard

Show therapists assessments needing review:

\`\`\`python
pending = service.get_pending_reviews(limit=50)
for assessment in pending:
    show_review_interface(assessment)
\`\`\`

### 4. AI Agent Integration

AI agents check confidence before using results:

\`\`\`python
prefs = service.get_communication_preferences(user_id)

if prefs["confidence"] < 60:
    agent.add_caveat("Assessment has low confidence")
    agent.request_reassessment()
else:
    agent.use_style(prefs["style"])
\`\`\`

## üß™ Testing

Run tests:

\`\`\`bash
python backend/test_enhanced_assessment.py
\`\`\`

Tests include:
- ‚úÖ Quality metric calculation
- ‚úÖ Pattern detection accuracy
- ‚úÖ Confidence scoring rules
- ‚úÖ Time-based penalties
- ‚úÖ Auto-flagging logic

## üìà Dashboard Integration

Build a clinical review dashboard:

\`\`\`javascript
// Fetch pending reviews
const pending = await fetch('/api/v1/ep-assessment/pending-reviews')

// Show in UI
pending.forEach(assessment => {
  renderReviewCard({
    user: assessment.user_id,
    confidence: assessment.quality_metrics.confidence_score,
    pattern: assessment.quality_metrics.answer_pattern,
    reasons: assessment.quality_metrics.review_reasons,
    actions: ['approve', 'reject', 'reassess']
  })
})
\`\`\`

## üîí Role-Based Access

- **Users**: Submit assessments, view their own results
- **Clinicians**: Review flagged assessments, approve/reject
- **Admins**: View analytics, manage quality thresholds

## üìä Analytics Queries

\`\`\`sql
-- Assessments by confidence range
SELECT 
  CASE 
    WHEN confidence_score >= 90 THEN 'Excellent'
    WHEN confidence_score >= 70 THEN 'Good'
    WHEN confidence_score >= 50 THEN 'Fair'
    ELSE 'Poor'
  END as quality,
  COUNT(*) as count
FROM user_assessments
GROUP BY quality;

-- Pattern distribution
SELECT 
  answer_pattern_signature,
  COUNT(*) as count,
  AVG(confidence_score) as avg_confidence
FROM user_assessments
GROUP BY answer_pattern_signature
ORDER BY count DESC;

-- Flagged assessments trend
SELECT 
  DATE(completed_at) as date,
  COUNT(*) as total,
  SUM(CASE WHEN confidence_score < 60 THEN 1 ELSE 0 END) as flagged,
  AVG(confidence_score) as avg_confidence
FROM user_assessments
WHERE completed_at >= NOW() - INTERVAL '30 days'
GROUP BY date
ORDER BY date;
\`\`\`

## üéì Best Practices

1. **Set Quality Thresholds**
   - Require confidence > 60 for AI personalization
   - Auto-reassess if confidence < 50

2. **Monitor Patterns**
   - Alert if "all_yes" rate > 5%
   - Investigate high "suspicious" rates

3. **Clinical Oversight**
   - Review flagged assessments within 24 hours
   - Document all clinical decisions

4. **User Education**
   - Explain importance of honest answers
   - Show completion time guidance (3-10 min ideal)

5. **Regular Audits**
   - Weekly quality statistics review
   - Monthly pattern analysis
   - Quarterly threshold adjustments

## üîÑ Migration

If you already have assessments without quality metrics:

\`\`\`python
# Backfill quality metrics
for assessment in old_assessments:
    quality = service.calculate_quality_metrics(
        assessment.answers,
        assessment.time_to_complete_seconds
    )
    
    assessment.confidence_score = quality.confidence_score
    assessment.answer_pattern_signature = quality.answer_pattern
    db.commit()
\`\`\`

## üéâ Benefits

- ‚úÖ Improved data quality
- ‚úÖ Fraud detection
- ‚úÖ Clinical oversight
- ‚úÖ Better AI personalization
- ‚úÖ Audit trail for compliance
- ‚úÖ Analytics for improvement

---

**Questions?** Check the code comments in:
- \`services/ep_assessment_service_enhanced.py\`
- \`routes/ep_assessment_enhanced.py\`
- \`schemas/assessment.py\`
"@ | Out-File -FilePath $DocsFile -Encoding UTF8

Write-Host "‚úÖ Created: $DocsFile" -ForegroundColor Green

# ============================================================================
# Summary
# ============================================================================

Write-Host "`n" -NoNewline
Write-Host "============================================================================" -ForegroundColor Green
Write-Host "‚úÖ E&P ASSESSMENT ENHANCED FEATURES INSTALLED!" -ForegroundColor Green
Write-Host "============================================================================" -ForegroundColor Green

Write-Host "`nüìã Files Created:" -ForegroundColor Cyan
Write-Host "  1Ô∏è‚É£  Enhanced Service: $EnhancedServiceFile"
Write-Host "  2Ô∏è‚É£  Enhanced Routes: $EnhancedRoutesFile"
Write-Host "  3Ô∏è‚É£  Enhanced Schemas: $EnhancedSchemasFile"
Write-Host "  4Ô∏è‚É£  Test File: $TestFile"
Write-Host "  5Ô∏è‚É£  Documentation: $DocsFile"

Write-Host "`nüéØ Enhanced Features:" -ForegroundColor Yellow
Write-Host "  ‚úÖ Automatic quality scoring (0-100 confidence)"
Write-Host "  ‚úÖ Pattern detection (all-yes, all-no, alternating)"
Write-Host "  ‚úÖ Auto-flagging of suspicious assessments"
Write-Host "  ‚úÖ Clinical review workflow"
Write-Host "  ‚úÖ Quality analytics & reporting"
Write-Host "  ‚úÖ Time tracking & validation"

Write-Host "`nüß™ Test the Features:" -ForegroundColor Yellow
Write-Host "  python $TestFile" -ForegroundColor Gray

Write-Host "`nüìö Read the Documentation:" -ForegroundColor Yellow
Write-Host "  $DocsFile" -ForegroundColor Gray

Write-Host "`nüí° Integration Tips:" -ForegroundColor Cyan
Write-Host "  1. Use confidence scores to validate AI personalization"
Write-Host "  2. Build clinical dashboard for pending reviews"
Write-Host "  3. Monitor quality analytics weekly"
Write-Host "  4. Set up alerts for high fraud patterns"

Write-Host "`n============================================================================`n" -ForegroundColor Green
